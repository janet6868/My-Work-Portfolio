{"cells":[{"cell_type":"markdown","metadata":{"id":"Oe2js4BMPc9W"},"source":["### What resource we need to Train this large data??: GPU"]},{"cell_type":"markdown","metadata":{"id":"1Z6Vn4bqNvLA"},"source":["### Completed Tasks:\n","\n","--> GEE Asset/Crop polygons\n","\n","--> polygon area estimation for each crop type data\n","\n","--> Plante/NICFI Pixel count for each crops, each polygons\n","\n","--> Graphic representation for area vs Pixel count"]},{"cell_type":"markdown","metadata":{"id":"o0EPLwkbNSX_"},"source":["### Next tasks:\n","\n","--> Cloud masking for Planet (Algorthms)\n","\n","--> Image Normalization: Scale the pixel values (e.g., 0-255) to a range that is typical for neural network inputs, such as 0-1 or -1 to 1.\n","\n","--> Data Augmentation: To increase the diversity of the training data and prevent overfitting, apply transformations like rotations, translations, scaling, and horizontal flipping.\n","\n","--> Patch Extraction: For high-resolution images, it might be necessary to create smaller, manageable patches. This makes the training process more efficient and helps in handling large images during deployment.\n","\n","--> DL (FCNN) Trainings\n","--> Parallel ML training"]},{"cell_type":"markdown","metadata":{"id":"6uHyL3LNf6f7"},"source":["### Reading corner about crop mapping in Senegal\n","SEN4STAT/ESA: https://www.esa-sen4stat.org/user-stories/senegal-prototype/\n","\n","EOSTAT/FAO: https://data.apps.fao.org/catalog/dataset/5c377b2b-3c2e-4b70-afd7-0c80900b68bb/resource/50bc9ff5-95d2-40cd-af12-6aee2cfcc4ae"]},{"cell_type":"markdown","metadata":{"id":"sgXJNMd2coya"},"source":["access to the data_storage.py: https://drive.google.com/file/d/1-6_x0L6_yxaj3oxwmGJoYbn6luBgcnwX/view?usp=drive_link\n","access to the code below as script: https://drive.google.com/file/d/1-9158gNZZzkJLlUvEiqUkq6S7cVNLMf4/view?usp=sharing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tKVguRub1wd_"},"outputs":[],"source":["import ee\n","# @title Authenticate to the Earth Engine servers\n","ee.Authenticate()\n","# Initialize the Earth Engine object with Google Cloud project ID\n","project_id = 'ee-janet' # change here\n","ee.Initialize(project=project_id)\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-J9KtDs82eTM","executionInfo":{"status":"ok","timestamp":1733429038701,"user_tz":0,"elapsed":30326,"user":{"displayName":"Janet Mumo MUTUKU","userId":"08778445167667830809"}},"outputId":"10d0fe51-ca5c-464d-d186-b941a10ac4b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2457,"status":"ok","timestamp":1733429137930,"user":{"displayName":"Janet Mumo MUTUKU","userId":"08778445167667830809"},"user_tz":0},"id":"v7Qm72MpXi0f","outputId":"60a4108b-0cda-4676-c824-8ea33d7e88e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import os\n","os.chdir('/content/drive/MyDrive/Crop Monitoring/crop_types_data/CSE_team') # change to your drive path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0aaqStUK6zab"},"outputs":[],"source":["# @title Lib imports:\n","#import ee\n","#print('Using EE version ', ee.__version__)\n","import folium\n","#print('Using Folium version ', folium.__version__)\n","from os import MFD_HUGE_1MB\n","import pandas as pd\n","import random\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from typing import Dict, Iterable, List, Tuple\n","#from google.colab import auth\n","import datetime as dt\n","import time\n","import geopandas as gpd\n","from shapely.geometry import shape, Polygon, MultiPolygon\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1dHneEdKPazw"},"outputs":[],"source":["#import these packages\n","!pip install rasterio rasterstats fiona pyogrio geopandas earthpy geemap -q"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mNMTVh9AdhyG","executionInfo":{"status":"ok","timestamp":1733429216552,"user_tz":0,"elapsed":26850,"user":{"displayName":"Janet Mumo MUTUKU","userId":"08778445167667830809"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ab01925f-ef8b-4d3e-a2fd-4d0c599b140a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of features: 5832\n","Processing features from 0 to 5000\n","Processing features from 5000 to 10000\n","(5827, 7)\n","<class 'geopandas.geodataframe.GeoDataFrame'>\n"]}],"source":["#@title Running the imported files\n","import scripts.data_storage as ds  # Importing the data storage script\n","import scripts.process_raw_data as prd\n"]},{"cell_type":"markdown","metadata":{"id":"k228xy6ZbMZ8"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ebGnyLw5Xt0s"},"outputs":[],"source":["#@title Clean the raw data for all years\n","# Process data for 2018, 2019, 2020, and 2023 algother\n","# you need to adjust these paths and parameters based on your data\n","\n","shapefile_directory = '/content/drive/MyDrive/Crop Monitoring/crop_types_data/CSE_team/data_2018_2023'  # Set this to the directory where your shapefiles are stored\n","batch_size = 5000  # Set this to the desired batch size\n","data_years = {\n","    '2018': ds.data_2018,\n","    '2019': ds.data_2019,\n","    '2020': ds.data_2020,\n","    '2023': ds.data_2023\n","}\n","\n","for year, collection in data_years.items():\n","    prd.fetch_and_process_features(collection, year, batch_size,shapefile_directory)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8FJE7U42Xtt-"},"outputs":[],"source":["#@title import the data as GEE asset per subclass\n","\n","# Dictionary with years and corresponding shapefile names\n","shapefiles = {\n","    2018: 'clean_raw_data_2018.shp',\n","    2019: 'clean_raw_data_2019.shp',\n","    2020: 'clean_raw_data_2020.shp',\n","    2023: 'clean_raw_data_2023.shp'\n","}\n","\n","# Process and export each subclass for each year\n","for year, shapefile_name in shapefiles.items():\n","    # Load the GeoDataFrame from the shapefile\n","    shapefile_path = f'{shapefile_directory}/{shapefile_name}'\n","    gdf = gpd.read_file(shapefile_path)\n","    gdf= gdf[gdf['ID'].notna()]#.notna() #remove na in 2020 data\n","    # Group by subclass and export each subclass\n","    for subclass, subclass_gdf in gdf.groupby('Sub_class'):\n","        print(f\"subclass_gdf for {year}:\", subclass_gdf.shape)\n","        try:\n","            prd.export_to_asset(subclass_gdf, year, subclass)\n","        except Exception as e:\n","            print(f\"Error exporting year {year}, subclass {subclass}: {str(e)}\")\n","\n","        # Add a delay between exports to avoid overwhelming the EE API\n","        time.sleep(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jQ4EBLR9vUm-"},"outputs":[],"source":["# @title Export data using Sentinel 2\n","# Dictionary with years and corresponding shapefile names\n","shapefile_directory = '/content/drive/MyDrive/ICRISAT/crop_type_classification/crop_types_data/clean_data_with_bands_s2'\n","# Define the FeatureCollections from data_storage.py\n","feature_collections = {\n","    '2018_Cereals': ds.clean_raw_data_2018_Cereals,\n","    '2018_Legumes': ds.clean_raw_data_2018_Legumes,\n","    '2018_Noncrop': ds.clean_raw_data_2018_Noncrop,\n","    '2018_Tree_Crops': ds.clean_raw_data_2018_Tree_Crops,\n","    '2018_Vegetables': ds.clean_raw_data_2018_Vegetables,\n","    '2019_Cereals': ds.clean_raw_data_2019_Cereals,\n","    '2019_Legumes': ds.clean_raw_data_2019_Legumes,\n","    '2019_Noncrop': ds.clean_raw_data_2019_Noncrop,\n","    '2019_Vegetables': ds.clean_raw_data_2019_Vegetables,\n","    '2020_Bare_Built_Up': ds.clean_raw_data_2020_Bare_Built_Up,\n","    '2020_Cereals': ds.clean_raw_data_2020_Cereals,\n","    '2020_Fallow': ds.clean_raw_data_2020_Fallow,\n","    '2020_Legumes': ds.clean_raw_data_2020_Legumes,\n","    '2020_Noncrop': ds.clean_raw_data_2020_Noncrop,\n","    '2020_Other_Vegetation': ds.clean_raw_data_2020_Other_Vegetation,\n","    '2020_Tree_Crops': ds.clean_raw_data_2020_Tree_Crops,\n","    '2020_Vegetables': ds.clean_raw_data_2020_Vegetables,\n","    '2023_Cereals': ds.clean_raw_data_2023_Cereals,\n","    '2023_Fallow': ds.clean_raw_data_2023_Fallow,\n","    '2023_Legumes': ds.clean_raw_data_2023_Legumes,\n","    '2023_Noncrop': ds.clean_raw_data_2023_Noncrop,\n","    '2023_Tree_Crops': ds.clean_raw_data_2023_Tree_Crops,\n","    '2023_Vegetables': ds.clean_raw_data_2023_Vegetables\n","}\n","# Define the months and indices for Sentinel-2 processing\n","months = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n","indices = ['NDVI']\n","\n","# Process and export each subclass for each year\n","for year, shapefile_name in shapefiles.items():\n","    # Load the GeoDataFrame from the shapefile\n","    shapefile_path = f'{shapefile_directory}/{shapefile_name}'\n","    gdf = gpd.read_file(shapefile_path)\n","    gdf = gdf[gdf['ID'].notna()]  # Remove rows with NaN values in the 'ID' column\n","\n","    # Group by subclass and export each subclass\n","    for subclass, subclass_gdf in gdf.groupby('Sub_class'):\n","        print(f\"subclass_gdf for {year}:\", subclass_gdf.shape)\n","        try:\n","            subclass_processor = esp.SubclassProcessor(subclass_df=subclass_gdf, year=year, months=months, indices=indices, subclass_name=subclass)\n","            subclass_processor.export_to_drive(description=f'Sentinel2_Processed_Data_{year}_{subclass}')\n","        except Exception as e:\n","            print(f\"Error exporting year {year}, subclass {subclass}: {str(e)}\")\n","\n","        # Add a delay between exports to avoid overwhelming the EE API\n","        time.sleep(10)"]},{"cell_type":"markdown","metadata":{"id":"LiscXwvq9KGX"},"source":["#### Optional: Exporting the shapefiles into an asset GEE"]},{"cell_type":"markdown","metadata":{"id":"NTNjGBopoF1u"},"source":["### Extracting remote sensing indices and normalized band values"]},{"cell_type":"markdown","metadata":{"id":"-F4DJtEZXRmE"},"source":["Planet NICFI"]},{"cell_type":"markdown","metadata":{"id":"g4SYuDKd84nw"},"source":["$`Biannual Collection`\n","   PS_Tropical_Normalized_Analytic_Biannual\n","\n","*  December 2015,  June 2016, December 2016, June 2017, December 2017,   June 2018,December 2018, June 2019, December 2019, June 2020\n","                              \n","                          \n","\n","$`Monthly Collection`\n","  PS_Tropical_Normalized_Analytic_Monthly\n","* September 2020, October 2020,November 2020, December 2020,January 2021,February 2021, March 2021,April 2021,May 2021,June 2021,July 2021,August 2021, September 2021,October 2021,November 2021,December 2021,January 2022, February 2022,March 2022,  April 2022\n","                       "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3z_WMDcY9IHi"},"outputs":[],"source":["# Set working directory\n","os.chdir('/content/drive/MyDrive/Crop Monitoring/crop_monitoring/crop_types_data')\n","\n","# Load shapefiles\n","shp_2018 = gpd.read_file('clean_data_no_bands/land_cover_2018.shp')\n","shp_2019 = gpd.read_file('clean_data_no_bands/land_cover_2019.shp')\n","shp_2020 = gpd.read_file('clean_data_no_bands/land_cover_2020.shp')\n","shp_2023 = gpd.read_file('clean_data_no_bands/land_cover_2023.shp')\n","\n","class PlanetNICFIProcessor:\n","    def __init__(self, year, date_ranges, indices, data_fc):\n","        self.year = year\n","        self.date_ranges = date_ranges  # List of tuples (start_date, end_date)\n","        self.indices = indices\n","        self.data_fc = data_fc\n","\n","    def calculate_ndvi(self, image):\n","        \"\"\"\n","        Calculate the NDVI for an image and add it as a band.\n","        \"\"\"\n","        return image.addBands(image.normalizedDifference(['N', 'R']).rename('NDVI'))\n","\n","    def normalize_bands(self, image, band_names, geometry):\n","        \"\"\"\n","        Normalize the specified bands of an image.\n","        \"\"\"\n","        def normalize_band(band_name):\n","            band = image.select(band_name).toFloat()\n","            min_max = band.reduceRegion(\n","                reducer=ee.Reducer.minMax(),\n","                geometry=geometry,\n","                scale=4.77,\n","                maxPixels=1e13\n","            )\n","            min_val = ee.Number(min_max.get(ee.String(band_name).cat('_min')))\n","            max_val = ee.Number(min_max.get(ee.String(band_name).cat('_max')))\n","\n","            # Check if min and max are valid\n","            is_valid = min_val.isNumber().And(max_val.isNumber()).And(max_val.neq(min_val))\n","\n","            normalized_band = ee.Image(ee.Algorithms.If(\n","                is_valid,\n","                band.subtract(min_val).divide(max_val.subtract(min_val)),\n","                band  # If normalization fails, return the original band\n","            )).rename(ee.String(band_name).cat('_norm'))\n","\n","            return normalized_band\n","\n","        normalized_bands = [normalize_band(band) for band in band_names]\n","        return image.addBands(ee.Image.cat(normalized_bands))\n","\n","    def process_all_months(self):\n","        \"\"\"\n","        Process the NICFI Planet imagery for all specified date ranges.\n","        \"\"\"\n","        processed_images = []\n","\n","        for start_date, end_date in self.date_ranges:\n","            print(f\"Processing period: {start_date} to {end_date}\")\n","\n","            nicfi_planet = ee.ImageCollection(\"projects/planet-nicfi/assets/basemaps/africa\") \\\n","                .filterBounds(self.data_fc.geometry()) \\\n","                .filter(ee.Filter.date(start_date, end_date))\n","\n","            # Check if the collection is empty\n","            count = nicfi_planet.size().getInfo()\n","            if count == 0:\n","                print(f\"No images found for the period: {start_date} to {end_date}\")\n","                continue\n","\n","            normalized_collection = nicfi_planet.map(lambda image: self.normalize_bands(image, ['B', 'G', 'R', 'N'], self.data_fc.geometry()))\n","            ndvi_collection = normalized_collection.map(self.calculate_ndvi)\n","\n","            print(f\"Number of images: {ndvi_collection.size().getInfo()}\")\n","\n","            bands = ['B_norm', 'G_norm', 'R_norm', 'N_norm', 'NDVI', 'B', 'G', 'R', 'N']\n","\n","            composite = ndvi_collection.median().select(bands)\n","\n","            # Add a date band to the composite\n","            date_band = ee.Image.constant(ee.Date(start_date).millis()).rename('date')\n","            composite_with_date = composite.addBands(date_band)\n","\n","            processed_images.append(composite_with_date)\n","\n","            time.sleep(random.uniform(5, 10))\n","\n","        if not processed_images:\n","            raise ValueError(\"No images were processed for any date range\")\n","\n","        return ee.ImageCollection(processed_images)\n","\n","    def add_bands_to_fc(self, image_collection):\n","        \"\"\"\n","        Add the calculated bands to the feature collection as time series.\n","        \"\"\"\n","        def sample_image_collection(feature):\n","            values = image_collection.map(lambda image: image.reduceRegion(\n","                reducer=ee.Reducer.mean(),\n","                geometry=feature.geometry(),\n","                scale=4.77,\n","                maxPixels=1e13\n","            )).toList(image_collection.size())\n","\n","            return feature.set('time_series', values)\n","\n","        return self.data_fc.map(sample_image_collection)\n","\n","    def export_to_drive(self, description, file_format='CSV', folder='clean_data_with_bands_planet_ncifi'):\n","        \"\"\"\n","        Export the feature collection with added bands to Google Drive.\n","        \"\"\"\n","        image_collection = self.process_all_months()\n","        data_with_bands = self.add_bands_to_fc(image_collection)\n","\n","        task = ee.batch.Export.table.toDrive(\n","            collection=data_with_bands,\n","            description=description,\n","            fileFormat=file_format,\n","            folder=folder\n","        )\n","        task.start()\n","        print(f\"Started export task: {task.id}\")\n","        print(\"Check your Earth Engine Tasks panel to monitor progress.\")\n","\n","class SubclassProcessor(PlanetNICFIProcessor):\n","    def __init__(self, subclass_df, year, date_ranges, indices, subclass_name):\n","        data_fc = gdf_to_ee_feature_collection(subclass_df)\n","        super().__init__(year, date_ranges, indices, data_fc)\n","        self.subclass_name = subclass_name\n","\n","def gdf_to_ee_feature_collection(gdf):\n","    \"\"\"\n","    Convert a GeoDataFrame to an Earth Engine Feature Collection.\n","    \"\"\"\n","    features = []\n","    for i, row in gdf.iterrows():\n","        geometry = row.geometry\n","        if isinstance(geometry, Polygon):\n","            ee_geometry = ee.Geometry.Polygon(list(geometry.exterior.coords))\n","        elif isinstance(geometry, MultiPolygon):\n","            polygons = [list(polygon.exterior.coords) for polygon in geometry.geoms]\n","            ee_geometry = ee.Geometry.MultiPolygon(polygons)\n","        else:\n","            raise TypeError(f\"Unsupported geometry type: {type(geometry)}\")\n","\n","        properties = row.drop('geometry').fillna(0).to_dict()  # Replace NaN with 0\n","        feature = ee.Feature(ee_geometry, properties)\n","        features.append(feature)\n","\n","    return ee.FeatureCollection(features)\n","\n","# Main execution\n","if __name__ == \"__main__\":\n","    # Set up your Google Drive folder\n","    os.chdir('/content/drive/MyDrive/Crop Monitoring/crop_monitoring/crop_types_data/data_with_bands_planet_ncifi')\n","\n","    # Years to process\n","    years = [2018, 2019, 2020, 2023]\n","\n","    # Define date ranges for each year\n","    date_ranges = {\n","        2018: [\n","            ('2018-06-01', '2018-08-31'),\n","            ('2018-06-01', '2018-09-30'),\n","            ('2018-06-01', '2018-10-31'),\n","            ('2018-06-01', '2018-11-30')\n","        ],\n","        2019: [\n","            ('2019-06-01', '2019-08-31'),\n","            ('2019-06-01', '2019-09-30'),\n","            ('2019-06-01', '2019-10-31'),\n","            ('2019-06-01', '2019-11-30')\n","        ],\n","        2020: [\n","            ('2020-08-01', '2020-08-31'),\n","            ('2020-09-01', '2020-09-30'),\n","            ('2020-10-01', '2020-10-31'),\n","            ('2020-11-01', '2020-11-30')\n","        ],\n","        2023: [\n","            ('2023-08-01', '2023-08-31'),\n","            ('2023-09-01', '2023-09-30'),\n","            ('2023-10-01', '2023-10-31'),\n","            ('2023-11-01', '2023-11-30')\n","        ]\n","    }\n","\n","    indices = ['NDVI']  # Add more indices as you wish\n","\n","    shp_data = {\n","        2018: shp_2018,\n","        2019: shp_2019,\n","        2020: shp_2020,\n","        2023: shp_2023\n","    }\n","\n","    # Process each year and each subclass separately\n","    for year in years:\n","        for subclass_name, subclass_df in shp_data[year].groupby('Sub_class'):\n","            processor = SubclassProcessor(subclass_df, year, date_ranges[year], indices, subclass_name)\n","            try:\n","                # Ensure the description is a string and doesn't contain any special formatting characters\n","                description = f\"data_{year}_with_bands_subclass_{subclass_name}\"\n","                description = description.replace(\"%\", \"\").replace(\"{\", \"\").replace(\"}\", \"\")\n","                processor.export_to_drive(description)\n","            except ee.ee_exception.EEException as e:\n","                print(f\"Error processing {year}, subclass {subclass_name}: {str(e)}\")\n","            except ValueError as e:\n","                print(f\"No data available for {year}, subclass {subclass_name}: {str(e)}\")\n","            except Exception as e:\n","                print(f\"Unexpected error processing {year}, subclass {subclass_name}: {str(e)}\")\n","\n","            # Add a delay between processing subclasses\n","            time.sleep(random.uniform(10, 20))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1QVnmVaMtzlzGiIEepppnzheQaSDTKa2Z","timestamp":1721701650686},{"file_id":"1YtJE8vuA8cAX53zdjesD1-rqACmXm-Y3","timestamp":1721519314165}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":0}